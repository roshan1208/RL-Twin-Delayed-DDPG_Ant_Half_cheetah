{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_project_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqbH_WFYIKcN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BChfPgUuLK-G"
      },
      "source": [
        "# Let get started "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl5qGujBLUEM",
        "outputId": "8c375e46-e2f2-40c6-b982-6b28dc4b1055"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/60b97ffc579db665bdd87f2cb47fe1215ae770fbbc1add84ebf36ddca63b/pybullet-3.1.7.tar.gz (79.0MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0MB 54kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89750959 sha256=75d462715213c9cfb6ab003bc7e0a77ee8a62b2936a3d95e6297de5e4fc9dafe\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/56/e6/fce8276a2f30165f7ac31089bb72f390fa16b87328651e1a5a\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNgV5jvTNgdn"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gkH1odrLfRV"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhkCXlLN10C"
      },
      "source": [
        "# **Step 1 :** We initialize the experience Reply memory (s, s', a, r, 0/1)\n",
        "##          s = current state\n",
        "##          s' = next state\n",
        "##          a = action\n",
        "##          r = reward\n",
        "##          done -> 0/1 , if 1 then episode is done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NivWuy4GOPrn"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []           # memory\n",
        "    self.max_size = max_size    # maximum size = 1 million\n",
        "    self.ptr = 0               # pointer or index of different cell of experience reply memory [(s1, s1', a1, r1, 0/1), (s2, s2', a2, r2, 0/1), -------] \n",
        "\n",
        "  def add(self, transition):              # add new transtion(s, s', a, r, 0/1) into the memory\n",
        "    if len(self.storage) == self.max_size:      # if memory is fully populated \n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1)%self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):      # some sample transition to create batches\n",
        "    ind = np.random.randint(0, len(self.storage), batch_size)   # generate \"batch_size\" no of random indexs\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []     #initalize batch\n",
        "    for i in ind:                                                # Create batches \n",
        "      state, next_state, action, reward, done = self.storage[i] \n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhWSUSz_aL-z"
      },
      "source": [
        "# **Step 2 :** Build one Neural Network for Actor model and one Neural Network for Actor Target \n",
        "\n",
        "## **Shape ::** input(=states) -> 400 Hidden neuron(Act=Relu) -> 300 Hidden neuron(Act=Relu) -> output(=actions)(Act=tanh)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-OiChJIZ5DS"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, max_action):     # max_action will use to clip actions\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)         # layer 1 = connection between input layer to hidden layer 1\n",
        "    self.layer_2 = nn.Linear(400, 300)               # layer 2 = connection between hidden layer 1 to hidden layer 2\n",
        "    self.layer_3 = nn.Linear(300, action_dim)        # layer 3 = connection between hidden layer 2 to output layer\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):        # Forward propogation in NN, x=input state\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE7A06w7ga1W"
      },
      "source": [
        "# **Step 3 :** Build two Neural Network for Critic model and two Neural Network for Critic Target \n",
        "\n",
        "## **Shape ::** input(=states + actions) -> 400 Hidden neuron(Act=Relu) -> 300 Hidden neuron(Act=Relu) -> output(= Q value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wMTGCUKf1My"
      },
      "source": [
        "class Critic(nn.Module):       # Twin part\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):       # x, u represent state and action respectively  \n",
        "    xu = torch.cat([x, u], 1)    # concatnate state and action\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):   # forward method that return only Q1 (for gradient ascent part)\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrO0wrq43up"
      },
      "source": [
        "## **Step 4 to 15:**  Training Process "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcLtP5APl3cX"
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")          #using cpu to train out model\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):             # dimension that change with respect to environment\n",
        "\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)    # create Actor model object from Actor class\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)   # create Actor target object from Actor class\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())                # to load our fully pre train model in future \n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())           # ADAM optimizer for actor\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)                  # create critic model object from Critic class\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)           # create critic target object from Critic class\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())       # ADAM optimizer for critic\n",
        "    \n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):           # take state as input , reshape it then return action to perform for actor\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)      # reshape state\n",
        "    return self.actor(state).cpu().data.numpy().flatten()      # return action \n",
        "\n",
        "  #----------------------------------------------------------------------------------------------------------------------------------------  \n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \"\"\"\n",
        "    reply_buffer : memory (s, s', a, r)\n",
        "    iterations : number of iteration of whole training process\n",
        "    batch_size : number of transition in each batch\n",
        "    discount : discount factor  \"gamma\"  (small discount)\n",
        "    tau : for poliyak averaging (for updating weight of the actor target )  ---- using same value as paper used\n",
        "    policy_noise : adding noise for exploration (S.D.=0.2 for gaussian distribution)\n",
        "    noise_clip : before adding sample noise in action clip that noise with value 0.5 (half)\n",
        "    policy_freq : frequence  of delay (for update weight of actor model, actor target and two critic target for every two iteration)\n",
        "    \"\"\"\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r, 0/1) from the memory and create 4 batch of size=100 of each element (from Step 1)\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      # Now convert it to torch Tensor format for torch module\n",
        "      state = torch.Tensor(batch_states).to(device)    \n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)       # calling actor target NN, forward method and returning action as output\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      # generate noise , clip it, add to next action and finally clip next action\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)   #calling critic target NN, forward method and returning Q1 and Q2\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()   # also consider \"done\" factor here for last episode\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)   #calling critic model NN, forward method and returning Q1 and Q2\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer (adam)\n",
        "      self.critic_optimizer.zero_grad()  # initilize gradient of the optimizer\n",
        "      critic_loss.backward()             # Compute gradient inside NN\n",
        "      self.critic_optimizer.step()       # perform SGD to update the parameter of two critic model\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model (Q1 method in step 3)\n",
        "      if it % policy_freq == 0:   # Delay part\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()  #(DPG part ) calculate gradient (diff. 1st critic model o/p Q1 wrt parameter of actor model )\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWYamix3Mnw5"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh8OMoopMopl"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJt_K5cIcvXO"
      },
      "source": [
        "## We set the parameters and create a file for the two saved models : Actors and Critics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yLf2AxNcesl",
        "outputId": "68a0db6a-caf5-4274-d4f9-32c883bc44f7"
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment we want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps (we can choose 1e6 also..)\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "\n",
        "\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "#We create a folder inside which will be saved the trained models\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLrIzzIJeENs"
      },
      "source": [
        "## **We create the PyBullet environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb4n9DCzeOwj",
        "outputId": "99a9c6ff-09fb-4d07-d462-f59755658473"
      },
      "source": [
        "env = gym.make(env_name)\n",
        "\n",
        "# We set seeds and we get the necessary information on the states and actions in the chosen environment\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnsrnGyPeqh0"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YNyx4TRfKRp",
        "outputId": "4a39d277-999c-452a-f3c5-d87f829fa64b"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "\n",
        "replay_buffer = ReplayBuffer()  # We create the Experience Replay memory\n",
        "\n",
        "evaluations = [evaluate_policy(policy)]  # We define a list where all the evaluation results over 10 episodes are stored"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KCyYvG-fclC"
      },
      "source": [
        " ## **We create a new folder directory in which the final results (videos of the agent) will be populated**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1DORcb7fjGJ"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asv1l03qfqqv"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWOroPYpfuZQ"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keeg3EUPfxAH"
      },
      "source": [
        "# **Training Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEU_1Bbgf0eP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff03e6f-55ad-44f9-9077-0c53cea1541d"
      },
      "source": [
        "Episode_num = []\n",
        "R_count = []\n",
        "\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:       # ( 0 to 500k)\n",
        "  \n",
        "  # If the episode is done (1000 timestamp = 1 episode)\n",
        "  if done:   \n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      Episode_num.append(episode_num)\n",
        "      R_count.append(episode_reward)\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)   # train our AI\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:      # Add noise for maximum exploration\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))   # Add (s, s' a, r, 0/1) into memory\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 525.4239200152978\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 512.2630104278476\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 511.1245591209761\n",
            "Total Timesteps: 3084 Episode Num: 4 Reward: 33.04315655652025\n",
            "Total Timesteps: 3297 Episode Num: 5 Reward: 94.2407502793168\n",
            "Total Timesteps: 4297 Episode Num: 6 Reward: 429.84755222078707\n",
            "Total Timesteps: 4525 Episode Num: 7 Reward: 105.04238549243016\n",
            "Total Timesteps: 4796 Episode Num: 8 Reward: 124.9333889783142\n",
            "Total Timesteps: 5796 Episode Num: 9 Reward: 532.6081862746242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 49.382863\n",
            "---------------------------------------\n",
            "Total Timesteps: 6796 Episode Num: 10 Reward: 491.5921389510269\n",
            "Total Timesteps: 7172 Episode Num: 11 Reward: 190.13347939136014\n",
            "Total Timesteps: 7370 Episode Num: 12 Reward: 87.92960302869034\n",
            "Total Timesteps: 8370 Episode Num: 13 Reward: 499.2595763203529\n",
            "Total Timesteps: 9370 Episode Num: 14 Reward: 513.337423164639\n",
            "Total Timesteps: 10059 Episode Num: 15 Reward: 308.3823311543886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -0.306056\n",
            "---------------------------------------\n",
            "Total Timesteps: 10177 Episode Num: 16 Reward: 8.291872830496825\n",
            "Total Timesteps: 10200 Episode Num: 17 Reward: -1.7935142220301907\n",
            "Total Timesteps: 10344 Episode Num: 18 Reward: 10.245901243533755\n",
            "Total Timesteps: 10475 Episode Num: 19 Reward: 9.372403776280342\n",
            "Total Timesteps: 10499 Episode Num: 20 Reward: -1.920596693278336\n",
            "Total Timesteps: 10521 Episode Num: 21 Reward: -5.242983698808943\n",
            "Total Timesteps: 10550 Episode Num: 22 Reward: 3.0143531181675227\n",
            "Total Timesteps: 10674 Episode Num: 23 Reward: 3.148556323592952\n",
            "Total Timesteps: 10697 Episode Num: 24 Reward: -3.640256012812077\n",
            "Total Timesteps: 10721 Episode Num: 25 Reward: -1.0728357638120698\n",
            "Total Timesteps: 10745 Episode Num: 26 Reward: -2.6410586286339575\n",
            "Total Timesteps: 10768 Episode Num: 27 Reward: -1.4496005586530638\n",
            "Total Timesteps: 10791 Episode Num: 28 Reward: -2.251399990193943\n",
            "Total Timesteps: 10882 Episode Num: 29 Reward: -1.3064895546371924\n",
            "Total Timesteps: 11013 Episode Num: 30 Reward: 6.763040414695334\n",
            "Total Timesteps: 11035 Episode Num: 31 Reward: -7.499882148683695\n",
            "Total Timesteps: 11097 Episode Num: 32 Reward: -2.9751234705305953\n",
            "Total Timesteps: 11194 Episode Num: 33 Reward: 5.108384493396153\n",
            "Total Timesteps: 11217 Episode Num: 34 Reward: -2.09903641133924\n",
            "Total Timesteps: 11241 Episode Num: 35 Reward: -0.6089475493896749\n",
            "Total Timesteps: 11264 Episode Num: 36 Reward: -3.09242457423418\n",
            "Total Timesteps: 11287 Episode Num: 37 Reward: -3.6709523032749116\n",
            "Total Timesteps: 11310 Episode Num: 38 Reward: -3.52950421943214\n",
            "Total Timesteps: 11425 Episode Num: 39 Reward: 5.9575600351169715\n",
            "Total Timesteps: 11448 Episode Num: 40 Reward: -4.36509146728783\n",
            "Total Timesteps: 11472 Episode Num: 41 Reward: -0.35047045965572343\n",
            "Total Timesteps: 11562 Episode Num: 42 Reward: 5.004045956143154\n",
            "Total Timesteps: 11691 Episode Num: 43 Reward: 6.013727697139634\n",
            "Total Timesteps: 11715 Episode Num: 44 Reward: -1.373861299869055\n",
            "Total Timesteps: 11764 Episode Num: 45 Reward: -0.7705191683040595\n",
            "Total Timesteps: 11869 Episode Num: 46 Reward: 6.362253431260429\n",
            "Total Timesteps: 11892 Episode Num: 47 Reward: -0.4926439651540854\n",
            "Total Timesteps: 11915 Episode Num: 48 Reward: -4.685158243772802\n",
            "Total Timesteps: 11939 Episode Num: 49 Reward: -0.17684709883802752\n",
            "Total Timesteps: 12135 Episode Num: 50 Reward: 12.784395205683388\n",
            "Total Timesteps: 12318 Episode Num: 51 Reward: 16.100738283636705\n",
            "Total Timesteps: 12394 Episode Num: 52 Reward: -1.4414554814041964\n",
            "Total Timesteps: 12498 Episode Num: 53 Reward: 5.4858936708767985\n",
            "Total Timesteps: 12521 Episode Num: 54 Reward: -1.0444505820470829\n",
            "Total Timesteps: 12598 Episode Num: 55 Reward: 4.921469767404052\n",
            "Total Timesteps: 12622 Episode Num: 56 Reward: -0.7361391496537353\n",
            "Total Timesteps: 12744 Episode Num: 57 Reward: 5.1290214938641885\n",
            "Total Timesteps: 12792 Episode Num: 58 Reward: -1.872146041971262\n",
            "Total Timesteps: 12902 Episode Num: 59 Reward: 4.575745590954268\n",
            "Total Timesteps: 13053 Episode Num: 60 Reward: 9.59442266766145\n",
            "Total Timesteps: 13152 Episode Num: 61 Reward: 5.317320941364185\n",
            "Total Timesteps: 13175 Episode Num: 62 Reward: -0.8753492653508362\n",
            "Total Timesteps: 13198 Episode Num: 63 Reward: -1.6225874499549868\n",
            "Total Timesteps: 13220 Episode Num: 64 Reward: -5.23232639039205\n",
            "Total Timesteps: 13395 Episode Num: 65 Reward: 11.712026067678144\n",
            "Total Timesteps: 13418 Episode Num: 66 Reward: -4.245627717268071\n",
            "Total Timesteps: 13441 Episode Num: 67 Reward: -2.0746919306394505\n",
            "Total Timesteps: 13464 Episode Num: 68 Reward: -4.0315078680356375\n",
            "Total Timesteps: 13486 Episode Num: 69 Reward: -5.17870005178052\n",
            "Total Timesteps: 13509 Episode Num: 70 Reward: -3.059870948103847\n",
            "Total Timesteps: 13592 Episode Num: 71 Reward: 4.096313106783471\n",
            "Total Timesteps: 13615 Episode Num: 72 Reward: -1.2057818562733373\n",
            "Total Timesteps: 13755 Episode Num: 73 Reward: 7.5197357807167595\n",
            "Total Timesteps: 13778 Episode Num: 74 Reward: -3.053983767469591\n",
            "Total Timesteps: 13906 Episode Num: 75 Reward: 8.004666159567384\n",
            "Total Timesteps: 13972 Episode Num: 76 Reward: 2.1706070086809794\n",
            "Total Timesteps: 13996 Episode Num: 77 Reward: -0.6001766822545661\n",
            "Total Timesteps: 14047 Episode Num: 78 Reward: 1.814413769869604\n",
            "Total Timesteps: 14117 Episode Num: 79 Reward: 1.6202087778533885\n",
            "Total Timesteps: 14140 Episode Num: 80 Reward: -2.6222166453137072\n",
            "Total Timesteps: 14329 Episode Num: 81 Reward: 14.860659022125393\n",
            "Total Timesteps: 14381 Episode Num: 82 Reward: 0.6801991014025208\n",
            "Total Timesteps: 14404 Episode Num: 83 Reward: -2.619770219973991\n",
            "Total Timesteps: 14469 Episode Num: 84 Reward: 0.06887492347567936\n",
            "Total Timesteps: 14515 Episode Num: 85 Reward: -0.4640670571112464\n",
            "Total Timesteps: 14575 Episode Num: 86 Reward: 0.3541242317659885\n",
            "Total Timesteps: 14598 Episode Num: 87 Reward: -2.0906065990435465\n",
            "Total Timesteps: 14621 Episode Num: 88 Reward: -3.9431178776849425\n",
            "Total Timesteps: 14748 Episode Num: 89 Reward: 5.111832880861596\n",
            "Total Timesteps: 14891 Episode Num: 90 Reward: 10.524786545551265\n",
            "Total Timesteps: 14996 Episode Num: 91 Reward: 5.2187280903029425\n",
            "Total Timesteps: 15020 Episode Num: 92 Reward: -0.29322538326186853\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1.195824\n",
            "---------------------------------------\n",
            "Total Timesteps: 15043 Episode Num: 93 Reward: -2.215612906279495\n",
            "Total Timesteps: 15171 Episode Num: 94 Reward: 5.326152024189052\n",
            "Total Timesteps: 15194 Episode Num: 95 Reward: -2.0434551333704754\n",
            "Total Timesteps: 15217 Episode Num: 96 Reward: -0.9955224311702044\n",
            "Total Timesteps: 15239 Episode Num: 97 Reward: -6.545073703829692\n",
            "Total Timesteps: 15262 Episode Num: 98 Reward: -2.8619268368467834\n",
            "Total Timesteps: 15384 Episode Num: 99 Reward: 3.395785686747895\n",
            "Total Timesteps: 15520 Episode Num: 100 Reward: 9.945855520385082\n",
            "Total Timesteps: 15543 Episode Num: 101 Reward: -2.4065504844869583\n",
            "Total Timesteps: 15566 Episode Num: 102 Reward: -4.377676979035007\n",
            "Total Timesteps: 15714 Episode Num: 103 Reward: 10.169012294115317\n",
            "Total Timesteps: 15737 Episode Num: 104 Reward: -3.514299877478435\n",
            "Total Timesteps: 15759 Episode Num: 105 Reward: -5.747083503210958\n",
            "Total Timesteps: 15782 Episode Num: 106 Reward: -3.059345426685323\n",
            "Total Timesteps: 15907 Episode Num: 107 Reward: 4.709874323406706\n",
            "Total Timesteps: 15930 Episode Num: 108 Reward: -6.791791558195927\n",
            "Total Timesteps: 16016 Episode Num: 109 Reward: 2.63053606318294\n",
            "Total Timesteps: 16038 Episode Num: 110 Reward: -6.783594145863059\n",
            "Total Timesteps: 16096 Episode Num: 111 Reward: 0.3253421740783855\n",
            "Total Timesteps: 16120 Episode Num: 112 Reward: -2.279927692640657\n",
            "Total Timesteps: 16143 Episode Num: 113 Reward: -1.3322538494940903\n",
            "Total Timesteps: 16166 Episode Num: 114 Reward: -1.432249882313436\n",
            "Total Timesteps: 16261 Episode Num: 115 Reward: 3.0048099080608086\n",
            "Total Timesteps: 16284 Episode Num: 116 Reward: -1.9868886902085046\n",
            "Total Timesteps: 16307 Episode Num: 117 Reward: -4.081263304651815\n",
            "Total Timesteps: 16330 Episode Num: 118 Reward: -3.1332554849282164\n",
            "Total Timesteps: 16400 Episode Num: 119 Reward: 4.3523313339124545\n",
            "Total Timesteps: 16498 Episode Num: 120 Reward: 3.9690051422958073\n",
            "Total Timesteps: 16566 Episode Num: 121 Reward: 0.03893363912051617\n",
            "Total Timesteps: 17566 Episode Num: 122 Reward: 191.2069037655194\n",
            "Total Timesteps: 18566 Episode Num: 123 Reward: 193.72385729762803\n",
            "Total Timesteps: 19566 Episode Num: 124 Reward: 171.07246334835173\n",
            "Total Timesteps: 20566 Episode Num: 125 Reward: 89.27792783805914\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.866803\n",
            "---------------------------------------\n",
            "Total Timesteps: 21566 Episode Num: 126 Reward: 91.69274677343793\n",
            "Total Timesteps: 22566 Episode Num: 127 Reward: 93.67688482705296\n",
            "Total Timesteps: 23566 Episode Num: 128 Reward: 95.61461822236379\n",
            "Total Timesteps: 24566 Episode Num: 129 Reward: 92.78937188504997\n",
            "Total Timesteps: 25566 Episode Num: 130 Reward: 91.50997930623322\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.318400\n",
            "---------------------------------------\n",
            "Total Timesteps: 26566 Episode Num: 131 Reward: 90.36440205745025\n",
            "Total Timesteps: 27566 Episode Num: 132 Reward: 92.89626278973853\n",
            "Total Timesteps: 28566 Episode Num: 133 Reward: 97.1991196956696\n",
            "Total Timesteps: 29566 Episode Num: 134 Reward: 93.88701227350286\n",
            "Total Timesteps: 30566 Episode Num: 135 Reward: 91.97995347766214\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.096915\n",
            "---------------------------------------\n",
            "Total Timesteps: 31566 Episode Num: 136 Reward: 92.35081312217169\n",
            "Total Timesteps: 32566 Episode Num: 137 Reward: 93.26200876206507\n",
            "Total Timesteps: 33566 Episode Num: 138 Reward: 92.42930641319911\n",
            "Total Timesteps: 34566 Episode Num: 139 Reward: 90.93318611728913\n",
            "Total Timesteps: 35566 Episode Num: 140 Reward: 95.89481728578308\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.679424\n",
            "---------------------------------------\n",
            "Total Timesteps: 36566 Episode Num: 141 Reward: 91.45286440802953\n",
            "Total Timesteps: 37566 Episode Num: 142 Reward: 94.33280269746497\n",
            "Total Timesteps: 38566 Episode Num: 143 Reward: 92.38020599155162\n",
            "Total Timesteps: 39566 Episode Num: 144 Reward: 90.55666952673366\n",
            "Total Timesteps: 40566 Episode Num: 145 Reward: 92.03299731748855\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.298360\n",
            "---------------------------------------\n",
            "Total Timesteps: 41566 Episode Num: 146 Reward: 106.7819538083832\n",
            "Total Timesteps: 42566 Episode Num: 147 Reward: 91.63145542185242\n",
            "Total Timesteps: 43566 Episode Num: 148 Reward: 88.93643997878806\n",
            "Total Timesteps: 44566 Episode Num: 149 Reward: 91.60083121079839\n",
            "Total Timesteps: 45566 Episode Num: 150 Reward: 92.77346748177183\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.297228\n",
            "---------------------------------------\n",
            "Total Timesteps: 46566 Episode Num: 151 Reward: 129.88756856900673\n",
            "Total Timesteps: 47566 Episode Num: 152 Reward: 94.1422308049485\n",
            "Total Timesteps: 48566 Episode Num: 153 Reward: 90.47523396215952\n",
            "Total Timesteps: 49566 Episode Num: 154 Reward: 95.80149751235044\n",
            "Total Timesteps: 50566 Episode Num: 155 Reward: 91.15266550562518\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.515775\n",
            "---------------------------------------\n",
            "Total Timesteps: 51566 Episode Num: 156 Reward: 92.43174891720217\n",
            "Total Timesteps: 52566 Episode Num: 157 Reward: 96.51649947342281\n",
            "Total Timesteps: 53566 Episode Num: 158 Reward: 89.52237574253364\n",
            "Total Timesteps: 54566 Episode Num: 159 Reward: 93.16669567856091\n",
            "Total Timesteps: 55566 Episode Num: 160 Reward: 92.68643946894865\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.429721\n",
            "---------------------------------------\n",
            "Total Timesteps: 56566 Episode Num: 161 Reward: 95.47609250996469\n",
            "Total Timesteps: 57566 Episode Num: 162 Reward: 141.16219977064029\n",
            "Total Timesteps: 58566 Episode Num: 163 Reward: 144.24530519476548\n",
            "Total Timesteps: 59566 Episode Num: 164 Reward: 94.26299078243147\n",
            "Total Timesteps: 60566 Episode Num: 165 Reward: 91.40837208972292\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.482726\n",
            "---------------------------------------\n",
            "Total Timesteps: 61566 Episode Num: 166 Reward: 94.40658006102836\n",
            "Total Timesteps: 62566 Episode Num: 167 Reward: 94.32518106427527\n",
            "Total Timesteps: 63566 Episode Num: 168 Reward: 93.58812662727622\n",
            "Total Timesteps: 64566 Episode Num: 169 Reward: 92.04901789629186\n",
            "Total Timesteps: 65566 Episode Num: 170 Reward: 95.96878640843126\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.998733\n",
            "---------------------------------------\n",
            "Total Timesteps: 66566 Episode Num: 171 Reward: 110.4332863260738\n",
            "Total Timesteps: 67566 Episode Num: 172 Reward: 91.31616843062326\n",
            "Total Timesteps: 68566 Episode Num: 173 Reward: 91.64575904407846\n",
            "Total Timesteps: 69566 Episode Num: 174 Reward: 93.93474108457146\n",
            "Total Timesteps: 70566 Episode Num: 175 Reward: 91.09107276968147\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.278874\n",
            "---------------------------------------\n",
            "Total Timesteps: 71566 Episode Num: 176 Reward: 93.90648312522056\n",
            "Total Timesteps: 72566 Episode Num: 177 Reward: 95.47643979874228\n",
            "Total Timesteps: 73566 Episode Num: 178 Reward: 92.08261429604374\n",
            "Total Timesteps: 74566 Episode Num: 179 Reward: 93.21193066586153\n",
            "Total Timesteps: 75566 Episode Num: 180 Reward: 90.85978722197677\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.907186\n",
            "---------------------------------------\n",
            "Total Timesteps: 76566 Episode Num: 181 Reward: 92.29187470570925\n",
            "Total Timesteps: 77566 Episode Num: 182 Reward: 96.25399722806914\n",
            "Total Timesteps: 78566 Episode Num: 183 Reward: 120.69948175224616\n",
            "Total Timesteps: 79566 Episode Num: 184 Reward: 92.9890789467188\n",
            "Total Timesteps: 80566 Episode Num: 185 Reward: 95.80918776158\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.714815\n",
            "---------------------------------------\n",
            "Total Timesteps: 81566 Episode Num: 186 Reward: 96.46215507281555\n",
            "Total Timesteps: 82566 Episode Num: 187 Reward: 93.72792356129956\n",
            "Total Timesteps: 83566 Episode Num: 188 Reward: 93.32906880819232\n",
            "Total Timesteps: 84566 Episode Num: 189 Reward: 90.2986016518844\n",
            "Total Timesteps: 85566 Episode Num: 190 Reward: 96.68267060001308\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.207262\n",
            "---------------------------------------\n",
            "Total Timesteps: 86566 Episode Num: 191 Reward: 95.79660473430589\n",
            "Total Timesteps: 87566 Episode Num: 192 Reward: 91.69253013129972\n",
            "Total Timesteps: 88566 Episode Num: 193 Reward: 90.17134464522097\n",
            "Total Timesteps: 89566 Episode Num: 194 Reward: 125.89644108262644\n",
            "Total Timesteps: 90566 Episode Num: 195 Reward: 93.37627640206154\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.485806\n",
            "---------------------------------------\n",
            "Total Timesteps: 91566 Episode Num: 196 Reward: 91.08947579647078\n",
            "Total Timesteps: 92566 Episode Num: 197 Reward: 94.3331174154164\n",
            "Total Timesteps: 93566 Episode Num: 198 Reward: 92.27504148633328\n",
            "Total Timesteps: 94566 Episode Num: 199 Reward: 92.10496409089622\n",
            "Total Timesteps: 95566 Episode Num: 200 Reward: 92.14177994861348\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.993714\n",
            "---------------------------------------\n",
            "Total Timesteps: 96566 Episode Num: 201 Reward: 94.6412190265604\n",
            "Total Timesteps: 97566 Episode Num: 202 Reward: 93.85543920347844\n",
            "Total Timesteps: 98566 Episode Num: 203 Reward: 89.3342429025264\n",
            "Total Timesteps: 99566 Episode Num: 204 Reward: 90.73733843781216\n",
            "Total Timesteps: 100566 Episode Num: 205 Reward: 92.25879587389825\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.151883\n",
            "---------------------------------------\n",
            "Total Timesteps: 101566 Episode Num: 206 Reward: 93.67807070300542\n",
            "Total Timesteps: 102566 Episode Num: 207 Reward: 93.8628092797511\n",
            "Total Timesteps: 103566 Episode Num: 208 Reward: 93.30253432092007\n",
            "Total Timesteps: 103970 Episode Num: 209 Reward: 77.53771647121835\n",
            "Total Timesteps: 104970 Episode Num: 210 Reward: 97.64736568646208\n",
            "Total Timesteps: 105970 Episode Num: 211 Reward: 91.6776308609375\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.319429\n",
            "---------------------------------------\n",
            "Total Timesteps: 106970 Episode Num: 212 Reward: 94.83760138996888\n",
            "Total Timesteps: 107970 Episode Num: 213 Reward: 95.1240408285983\n",
            "Total Timesteps: 108970 Episode Num: 214 Reward: 138.19413759292826\n",
            "Total Timesteps: 109970 Episode Num: 215 Reward: 94.23847313506768\n",
            "Total Timesteps: 110970 Episode Num: 216 Reward: 97.73840399490497\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.154542\n",
            "---------------------------------------\n",
            "Total Timesteps: 111970 Episode Num: 217 Reward: 91.25207061319834\n",
            "Total Timesteps: 112970 Episode Num: 218 Reward: 91.72493006131404\n",
            "Total Timesteps: 113970 Episode Num: 219 Reward: 93.96251588758423\n",
            "Total Timesteps: 114970 Episode Num: 220 Reward: 90.81790216005659\n",
            "Total Timesteps: 115970 Episode Num: 221 Reward: 93.89179315853765\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.749381\n",
            "---------------------------------------\n",
            "Total Timesteps: 116970 Episode Num: 222 Reward: 94.98069034636937\n",
            "Total Timesteps: 117970 Episode Num: 223 Reward: 92.16203407237526\n",
            "Total Timesteps: 118970 Episode Num: 224 Reward: 97.99040136903318\n",
            "Total Timesteps: 119970 Episode Num: 225 Reward: 92.06624419782555\n",
            "Total Timesteps: 120970 Episode Num: 226 Reward: 96.16251195340857\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 99.628549\n",
            "---------------------------------------\n",
            "Total Timesteps: 121970 Episode Num: 227 Reward: 93.09242380164847\n",
            "Total Timesteps: 122970 Episode Num: 228 Reward: 89.50482059880592\n",
            "Total Timesteps: 123970 Episode Num: 229 Reward: 92.74486605024445\n",
            "Total Timesteps: 124970 Episode Num: 230 Reward: 93.8656001602772\n",
            "Total Timesteps: 125970 Episode Num: 231 Reward: 93.62655009325587\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.943790\n",
            "---------------------------------------\n",
            "Total Timesteps: 126970 Episode Num: 232 Reward: 91.38166896850585\n",
            "Total Timesteps: 127970 Episode Num: 233 Reward: 131.97610002600845\n",
            "Total Timesteps: 128970 Episode Num: 234 Reward: 92.50905678659413\n",
            "Total Timesteps: 129970 Episode Num: 235 Reward: 92.37036200241644\n",
            "Total Timesteps: 130970 Episode Num: 236 Reward: 90.37110050651059\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.751635\n",
            "---------------------------------------\n",
            "Total Timesteps: 131970 Episode Num: 237 Reward: 89.16565912827241\n",
            "Total Timesteps: 132970 Episode Num: 238 Reward: 94.40706640382018\n",
            "Total Timesteps: 133970 Episode Num: 239 Reward: 115.66533498216829\n",
            "Total Timesteps: 134970 Episode Num: 240 Reward: 118.42163695890528\n",
            "Total Timesteps: 135970 Episode Num: 241 Reward: 89.98768026264663\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.722998\n",
            "---------------------------------------\n",
            "Total Timesteps: 136970 Episode Num: 242 Reward: 95.40490203468998\n",
            "Total Timesteps: 137970 Episode Num: 243 Reward: 93.26260564224022\n",
            "Total Timesteps: 138970 Episode Num: 244 Reward: 94.33200683158927\n",
            "Total Timesteps: 139970 Episode Num: 245 Reward: 91.17128101115175\n",
            "Total Timesteps: 140970 Episode Num: 246 Reward: 91.95516011122513\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.968911\n",
            "---------------------------------------\n",
            "Total Timesteps: 141970 Episode Num: 247 Reward: 131.5733287913643\n",
            "Total Timesteps: 142970 Episode Num: 248 Reward: 102.60118348225436\n",
            "Total Timesteps: 143970 Episode Num: 249 Reward: 91.41405629429808\n",
            "Total Timesteps: 144970 Episode Num: 250 Reward: 90.88428806078737\n",
            "Total Timesteps: 145970 Episode Num: 251 Reward: 94.4051983909273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.661946\n",
            "---------------------------------------\n",
            "Total Timesteps: 146970 Episode Num: 252 Reward: 94.86200790931163\n",
            "Total Timesteps: 147970 Episode Num: 253 Reward: 93.63266290914122\n",
            "Total Timesteps: 148970 Episode Num: 254 Reward: 91.88468046382147\n",
            "Total Timesteps: 149970 Episode Num: 255 Reward: 96.72000301239126\n",
            "Total Timesteps: 150970 Episode Num: 256 Reward: 92.7090597495394\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.629366\n",
            "---------------------------------------\n",
            "Total Timesteps: 151970 Episode Num: 257 Reward: 89.7462531852394\n",
            "Total Timesteps: 152970 Episode Num: 258 Reward: 92.51042265119021\n",
            "Total Timesteps: 153970 Episode Num: 259 Reward: 91.83125090340039\n",
            "Total Timesteps: 154970 Episode Num: 260 Reward: 93.43338989200619\n",
            "Total Timesteps: 155970 Episode Num: 261 Reward: 94.37477569805834\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.879635\n",
            "---------------------------------------\n",
            "Total Timesteps: 156970 Episode Num: 262 Reward: 92.13825515558919\n",
            "Total Timesteps: 157970 Episode Num: 263 Reward: 91.77860234541296\n",
            "Total Timesteps: 158970 Episode Num: 264 Reward: 93.01893133609677\n",
            "Total Timesteps: 159970 Episode Num: 265 Reward: 134.49968499307005\n",
            "Total Timesteps: 160970 Episode Num: 266 Reward: 92.12592531765976\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.451034\n",
            "---------------------------------------\n",
            "Total Timesteps: 161970 Episode Num: 267 Reward: 89.40373918256722\n",
            "Total Timesteps: 162970 Episode Num: 268 Reward: 92.00239404105504\n",
            "Total Timesteps: 163970 Episode Num: 269 Reward: 94.20502204172243\n",
            "Total Timesteps: 164970 Episode Num: 270 Reward: 90.89213295574206\n",
            "Total Timesteps: 165970 Episode Num: 271 Reward: 93.16647172721503\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.102714\n",
            "---------------------------------------\n",
            "Total Timesteps: 166970 Episode Num: 272 Reward: 98.06101797980489\n",
            "Total Timesteps: 167970 Episode Num: 273 Reward: 93.16812639428514\n",
            "Total Timesteps: 168970 Episode Num: 274 Reward: 90.33488650953615\n",
            "Total Timesteps: 169970 Episode Num: 275 Reward: 95.47092386770113\n",
            "Total Timesteps: 170970 Episode Num: 276 Reward: 95.95603631990018\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.862500\n",
            "---------------------------------------\n",
            "Total Timesteps: 171970 Episode Num: 277 Reward: 97.50243847595183\n",
            "Total Timesteps: 172970 Episode Num: 278 Reward: 97.73666501324915\n",
            "Total Timesteps: 173970 Episode Num: 279 Reward: 96.81781640521278\n",
            "Total Timesteps: 174970 Episode Num: 280 Reward: 95.90355788609618\n",
            "Total Timesteps: 175970 Episode Num: 281 Reward: 90.86879331943506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.368691\n",
            "---------------------------------------\n",
            "Total Timesteps: 176970 Episode Num: 282 Reward: 91.4879575776301\n",
            "Total Timesteps: 177970 Episode Num: 283 Reward: 92.80423763118178\n",
            "Total Timesteps: 178970 Episode Num: 284 Reward: 94.33610899464023\n",
            "Total Timesteps: 179970 Episode Num: 285 Reward: 91.84223267781387\n",
            "Total Timesteps: 180970 Episode Num: 286 Reward: 92.30256504836753\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.314045\n",
            "---------------------------------------\n",
            "Total Timesteps: 181970 Episode Num: 287 Reward: 93.97200254604773\n",
            "Total Timesteps: 182970 Episode Num: 288 Reward: 94.68356614542965\n",
            "Total Timesteps: 183970 Episode Num: 289 Reward: 94.7940709461725\n",
            "Total Timesteps: 184970 Episode Num: 290 Reward: 91.78616667977899\n",
            "Total Timesteps: 185970 Episode Num: 291 Reward: 97.90360765163034\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.662206\n",
            "---------------------------------------\n",
            "Total Timesteps: 186970 Episode Num: 292 Reward: 92.64864458103743\n",
            "Total Timesteps: 187970 Episode Num: 293 Reward: 92.85057427994569\n",
            "Total Timesteps: 188970 Episode Num: 294 Reward: 93.49798457926248\n",
            "Total Timesteps: 189970 Episode Num: 295 Reward: 94.55729548840651\n",
            "Total Timesteps: 190970 Episode Num: 296 Reward: 93.32329569695885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.498776\n",
            "---------------------------------------\n",
            "Total Timesteps: 191970 Episode Num: 297 Reward: 92.20327377220353\n",
            "Total Timesteps: 192970 Episode Num: 298 Reward: 92.69996278202761\n",
            "Total Timesteps: 193970 Episode Num: 299 Reward: 92.67283388082346\n",
            "Total Timesteps: 194970 Episode Num: 300 Reward: 92.0727951864465\n",
            "Total Timesteps: 195970 Episode Num: 301 Reward: 129.07882479766891\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.470722\n",
            "---------------------------------------\n",
            "Total Timesteps: 196970 Episode Num: 302 Reward: 92.59454520766734\n",
            "Total Timesteps: 197970 Episode Num: 303 Reward: 93.42746327720349\n",
            "Total Timesteps: 198970 Episode Num: 304 Reward: 94.18798188632128\n",
            "Total Timesteps: 199970 Episode Num: 305 Reward: 93.14935937810776\n",
            "Total Timesteps: 200970 Episode Num: 306 Reward: 91.62585824273253\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 92.112711\n",
            "---------------------------------------\n",
            "Total Timesteps: 201970 Episode Num: 307 Reward: 100.56954526318182\n",
            "Total Timesteps: 202970 Episode Num: 308 Reward: 93.29641674500934\n",
            "Total Timesteps: 203970 Episode Num: 309 Reward: 93.88774907364004\n",
            "Total Timesteps: 204970 Episode Num: 310 Reward: 93.55832391596657\n",
            "Total Timesteps: 205970 Episode Num: 311 Reward: 93.20275132786573\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.518215\n",
            "---------------------------------------\n",
            "Total Timesteps: 206970 Episode Num: 312 Reward: 90.47076079734663\n",
            "Total Timesteps: 207970 Episode Num: 313 Reward: 99.86628997424042\n",
            "Total Timesteps: 208970 Episode Num: 314 Reward: 93.3492262828818\n",
            "Total Timesteps: 209970 Episode Num: 315 Reward: 87.14274187952371\n",
            "Total Timesteps: 210970 Episode Num: 316 Reward: 140.11664287899774\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.254539\n",
            "---------------------------------------\n",
            "Total Timesteps: 211970 Episode Num: 317 Reward: 96.11449579565998\n",
            "Total Timesteps: 212970 Episode Num: 318 Reward: 93.84558870889227\n",
            "Total Timesteps: 213970 Episode Num: 319 Reward: 91.48067000125089\n",
            "Total Timesteps: 214970 Episode Num: 320 Reward: 96.19595894770511\n",
            "Total Timesteps: 215970 Episode Num: 321 Reward: 96.60978372301108\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.633171\n",
            "---------------------------------------\n",
            "Total Timesteps: 216970 Episode Num: 322 Reward: 94.85541936281373\n",
            "Total Timesteps: 217970 Episode Num: 323 Reward: 93.90329904017885\n",
            "Total Timesteps: 218970 Episode Num: 324 Reward: 93.92784953619493\n",
            "Total Timesteps: 219970 Episode Num: 325 Reward: 93.97267104539522\n",
            "Total Timesteps: 220970 Episode Num: 326 Reward: 90.07105119400929\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.416281\n",
            "---------------------------------------\n",
            "Total Timesteps: 221970 Episode Num: 327 Reward: 93.60707057246472\n",
            "Total Timesteps: 222970 Episode Num: 328 Reward: 99.5035792802794\n",
            "Total Timesteps: 223970 Episode Num: 329 Reward: 93.92613421440421\n",
            "Total Timesteps: 224970 Episode Num: 330 Reward: 96.22110926073144\n",
            "Total Timesteps: 225970 Episode Num: 331 Reward: 94.37621323336181\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.806969\n",
            "---------------------------------------\n",
            "Total Timesteps: 226970 Episode Num: 332 Reward: 93.08475497275305\n",
            "Total Timesteps: 227970 Episode Num: 333 Reward: 105.79690876203746\n",
            "Total Timesteps: 228970 Episode Num: 334 Reward: 92.94659483606101\n",
            "Total Timesteps: 229970 Episode Num: 335 Reward: 92.75603818866037\n",
            "Total Timesteps: 230970 Episode Num: 336 Reward: 93.47217698510276\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.930451\n",
            "---------------------------------------\n",
            "Total Timesteps: 231970 Episode Num: 337 Reward: 93.89048471317685\n",
            "Total Timesteps: 232970 Episode Num: 338 Reward: 90.98929585406185\n",
            "Total Timesteps: 233970 Episode Num: 339 Reward: 91.13734405637199\n",
            "Total Timesteps: 234970 Episode Num: 340 Reward: 92.0717301234406\n",
            "Total Timesteps: 235970 Episode Num: 341 Reward: 92.73207688854875\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.644864\n",
            "---------------------------------------\n",
            "Total Timesteps: 236970 Episode Num: 342 Reward: 91.49013994295485\n",
            "Total Timesteps: 237970 Episode Num: 343 Reward: 91.24084473341182\n",
            "Total Timesteps: 238970 Episode Num: 344 Reward: 90.20490858372315\n",
            "Total Timesteps: 239970 Episode Num: 345 Reward: 93.03262439881739\n",
            "Total Timesteps: 240970 Episode Num: 346 Reward: 98.07379454951881\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.731398\n",
            "---------------------------------------\n",
            "Total Timesteps: 241970 Episode Num: 347 Reward: 93.08793142140276\n",
            "Total Timesteps: 242970 Episode Num: 348 Reward: 95.11673515150741\n",
            "Total Timesteps: 243970 Episode Num: 349 Reward: 96.35762129250864\n",
            "Total Timesteps: 244970 Episode Num: 350 Reward: 93.71125240150894\n",
            "Total Timesteps: 245970 Episode Num: 351 Reward: 91.62494594437872\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.563612\n",
            "---------------------------------------\n",
            "Total Timesteps: 246970 Episode Num: 352 Reward: 88.96908952800727\n",
            "Total Timesteps: 247970 Episode Num: 353 Reward: 94.11307502502041\n",
            "Total Timesteps: 248423 Episode Num: 354 Reward: 83.98959504101876\n",
            "Total Timesteps: 249423 Episode Num: 355 Reward: 95.46469719656251\n",
            "Total Timesteps: 250423 Episode Num: 356 Reward: 92.78498789866879\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.767148\n",
            "---------------------------------------\n",
            "Total Timesteps: 251423 Episode Num: 357 Reward: 119.69198593300082\n",
            "Total Timesteps: 252423 Episode Num: 358 Reward: 93.17256063157848\n",
            "Total Timesteps: 253423 Episode Num: 359 Reward: 92.22643177558595\n",
            "Total Timesteps: 254423 Episode Num: 360 Reward: 91.72282104399912\n",
            "Total Timesteps: 255423 Episode Num: 361 Reward: 118.01006129180456\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.741500\n",
            "---------------------------------------\n",
            "Total Timesteps: 256423 Episode Num: 362 Reward: 92.05594153705319\n",
            "Total Timesteps: 257423 Episode Num: 363 Reward: 93.33674562164435\n",
            "Total Timesteps: 258423 Episode Num: 364 Reward: 93.39831252570335\n",
            "Total Timesteps: 259423 Episode Num: 365 Reward: 94.52247167828011\n",
            "Total Timesteps: 260423 Episode Num: 366 Reward: 90.41820998979745\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.366112\n",
            "---------------------------------------\n",
            "Total Timesteps: 261423 Episode Num: 367 Reward: 92.81380635896838\n",
            "Total Timesteps: 262423 Episode Num: 368 Reward: 92.42857385446584\n",
            "Total Timesteps: 263423 Episode Num: 369 Reward: 90.94364418908663\n",
            "Total Timesteps: 264423 Episode Num: 370 Reward: 96.50909067765164\n",
            "Total Timesteps: 265423 Episode Num: 371 Reward: 95.00089452915432\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.485089\n",
            "---------------------------------------\n",
            "Total Timesteps: 266423 Episode Num: 372 Reward: 95.66257091013136\n",
            "Total Timesteps: 267423 Episode Num: 373 Reward: 96.39096943550594\n",
            "Total Timesteps: 268423 Episode Num: 374 Reward: 132.4980478582257\n",
            "Total Timesteps: 269423 Episode Num: 375 Reward: 93.55257447103628\n",
            "Total Timesteps: 270423 Episode Num: 376 Reward: 95.24394239889781\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 91.042635\n",
            "---------------------------------------\n",
            "Total Timesteps: 271423 Episode Num: 377 Reward: 91.6617896639777\n",
            "Total Timesteps: 272423 Episode Num: 378 Reward: 92.37507282123057\n",
            "Total Timesteps: 273423 Episode Num: 379 Reward: 95.35917815719486\n",
            "Total Timesteps: 274423 Episode Num: 380 Reward: 95.53235664049511\n",
            "Total Timesteps: 275423 Episode Num: 381 Reward: 94.62178514528435\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.222607\n",
            "---------------------------------------\n",
            "Total Timesteps: 276423 Episode Num: 382 Reward: 105.9320101611635\n",
            "Total Timesteps: 277423 Episode Num: 383 Reward: 94.5669502862959\n",
            "Total Timesteps: 278423 Episode Num: 384 Reward: 89.58548175838085\n",
            "Total Timesteps: 279423 Episode Num: 385 Reward: 109.64484395172006\n",
            "Total Timesteps: 280423 Episode Num: 386 Reward: 89.53518720227252\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.556514\n",
            "---------------------------------------\n",
            "Total Timesteps: 281423 Episode Num: 387 Reward: 95.38168327100364\n",
            "Total Timesteps: 282423 Episode Num: 388 Reward: 93.1736866429781\n",
            "Total Timesteps: 283423 Episode Num: 389 Reward: 89.83072904119824\n",
            "Total Timesteps: 284423 Episode Num: 390 Reward: 106.9952977428602\n",
            "Total Timesteps: 285423 Episode Num: 391 Reward: 95.90559088283297\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.835470\n",
            "---------------------------------------\n",
            "Total Timesteps: 286423 Episode Num: 392 Reward: 137.8355094275841\n",
            "Total Timesteps: 287423 Episode Num: 393 Reward: 94.82622348032325\n",
            "Total Timesteps: 288423 Episode Num: 394 Reward: 97.21693288375832\n",
            "Total Timesteps: 289423 Episode Num: 395 Reward: 96.46346906413957\n",
            "Total Timesteps: 290423 Episode Num: 396 Reward: 136.5511302216611\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.417449\n",
            "---------------------------------------\n",
            "Total Timesteps: 291423 Episode Num: 397 Reward: 93.62119105784345\n",
            "Total Timesteps: 292423 Episode Num: 398 Reward: 96.62202025829666\n",
            "Total Timesteps: 293423 Episode Num: 399 Reward: 91.2761236199228\n",
            "Total Timesteps: 294423 Episode Num: 400 Reward: 97.11550781180588\n",
            "Total Timesteps: 295423 Episode Num: 401 Reward: 93.06943542776763\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.536673\n",
            "---------------------------------------\n",
            "Total Timesteps: 296423 Episode Num: 402 Reward: 91.87537859627056\n",
            "Total Timesteps: 297423 Episode Num: 403 Reward: 92.63668241135557\n",
            "Total Timesteps: 298423 Episode Num: 404 Reward: 93.80311123260203\n",
            "Total Timesteps: 299423 Episode Num: 405 Reward: 112.01439402768266\n",
            "Total Timesteps: 300423 Episode Num: 406 Reward: 95.55785956143524\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.537444\n",
            "---------------------------------------\n",
            "Total Timesteps: 301423 Episode Num: 407 Reward: 95.04166677135571\n",
            "Total Timesteps: 302423 Episode Num: 408 Reward: 94.07403745589266\n",
            "Total Timesteps: 303423 Episode Num: 409 Reward: 92.3000287461564\n",
            "Total Timesteps: 304423 Episode Num: 410 Reward: 93.2456396725359\n",
            "Total Timesteps: 305423 Episode Num: 411 Reward: 136.9576251159044\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.022510\n",
            "---------------------------------------\n",
            "Total Timesteps: 306423 Episode Num: 412 Reward: 94.68148412798627\n",
            "Total Timesteps: 307423 Episode Num: 413 Reward: 93.68701233617738\n",
            "Total Timesteps: 308423 Episode Num: 414 Reward: 141.79675690426157\n",
            "Total Timesteps: 309423 Episode Num: 415 Reward: 92.00473809621583\n",
            "Total Timesteps: 310423 Episode Num: 416 Reward: 145.0859208570123\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.066801\n",
            "---------------------------------------\n",
            "Total Timesteps: 311423 Episode Num: 417 Reward: 90.83771634602115\n",
            "Total Timesteps: 312423 Episode Num: 418 Reward: 90.83416525992341\n",
            "Total Timesteps: 313423 Episode Num: 419 Reward: 93.06919055966709\n",
            "Total Timesteps: 314423 Episode Num: 420 Reward: 91.56809149578572\n",
            "Total Timesteps: 315423 Episode Num: 421 Reward: 91.62042197417789\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.029513\n",
            "---------------------------------------\n",
            "Total Timesteps: 316423 Episode Num: 422 Reward: 108.82596264034923\n",
            "Total Timesteps: 317423 Episode Num: 423 Reward: 93.32914613502953\n",
            "Total Timesteps: 318423 Episode Num: 424 Reward: 92.13695224118408\n",
            "Total Timesteps: 319423 Episode Num: 425 Reward: 110.86050674386766\n",
            "Total Timesteps: 320423 Episode Num: 426 Reward: 106.24167070163008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.017452\n",
            "---------------------------------------\n",
            "Total Timesteps: 321423 Episode Num: 427 Reward: 92.54545572965588\n",
            "Total Timesteps: 322423 Episode Num: 428 Reward: 90.51168903514962\n",
            "Total Timesteps: 323423 Episode Num: 429 Reward: 91.92679799217777\n",
            "Total Timesteps: 324423 Episode Num: 430 Reward: 132.92522516406873\n",
            "Total Timesteps: 325423 Episode Num: 431 Reward: 93.79088375262525\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.041626\n",
            "---------------------------------------\n",
            "Total Timesteps: 326423 Episode Num: 432 Reward: 138.12252821546537\n",
            "Total Timesteps: 327423 Episode Num: 433 Reward: 91.32114224600559\n",
            "Total Timesteps: 328423 Episode Num: 434 Reward: 92.36181100759121\n",
            "Total Timesteps: 329423 Episode Num: 435 Reward: 95.28223446789136\n",
            "Total Timesteps: 330423 Episode Num: 436 Reward: 135.93331718744875\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.900982\n",
            "---------------------------------------\n",
            "Total Timesteps: 331423 Episode Num: 437 Reward: 90.39441288201778\n",
            "Total Timesteps: 332423 Episode Num: 438 Reward: 93.45437239920773\n",
            "Total Timesteps: 333423 Episode Num: 439 Reward: 94.18689104761198\n",
            "Total Timesteps: 334423 Episode Num: 440 Reward: 131.1702237076704\n",
            "Total Timesteps: 335423 Episode Num: 441 Reward: 88.96574996257233\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.907935\n",
            "---------------------------------------\n",
            "Total Timesteps: 336423 Episode Num: 442 Reward: 94.4088212179947\n",
            "Total Timesteps: 337423 Episode Num: 443 Reward: 88.90708766076052\n",
            "Total Timesteps: 338423 Episode Num: 444 Reward: 93.9699445661314\n",
            "Total Timesteps: 339423 Episode Num: 445 Reward: 94.07467934042697\n",
            "Total Timesteps: 340423 Episode Num: 446 Reward: 92.59498948191508\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.150593\n",
            "---------------------------------------\n",
            "Total Timesteps: 341423 Episode Num: 447 Reward: 95.30902219524096\n",
            "Total Timesteps: 342423 Episode Num: 448 Reward: 114.79312317961403\n",
            "Total Timesteps: 343423 Episode Num: 449 Reward: 91.4204477464904\n",
            "Total Timesteps: 344423 Episode Num: 450 Reward: 95.57389082834048\n",
            "Total Timesteps: 345423 Episode Num: 451 Reward: 95.87115883185956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.353135\n",
            "---------------------------------------\n",
            "Total Timesteps: 346423 Episode Num: 452 Reward: 93.19848275959292\n",
            "Total Timesteps: 347423 Episode Num: 453 Reward: 92.87073246076156\n",
            "Total Timesteps: 348423 Episode Num: 454 Reward: 93.81468033023602\n",
            "Total Timesteps: 349423 Episode Num: 455 Reward: 92.3501331034638\n",
            "Total Timesteps: 350423 Episode Num: 456 Reward: 94.12328845463328\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.365886\n",
            "---------------------------------------\n",
            "Total Timesteps: 351423 Episode Num: 457 Reward: 95.15881421608326\n",
            "Total Timesteps: 352423 Episode Num: 458 Reward: 93.36811708512813\n",
            "Total Timesteps: 353423 Episode Num: 459 Reward: 93.89210192746978\n",
            "Total Timesteps: 354423 Episode Num: 460 Reward: 116.68041843945636\n",
            "Total Timesteps: 355423 Episode Num: 461 Reward: 91.53659829460918\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.368814\n",
            "---------------------------------------\n",
            "Total Timesteps: 356423 Episode Num: 462 Reward: 90.95251433967772\n",
            "Total Timesteps: 357423 Episode Num: 463 Reward: 95.47825830904355\n",
            "Total Timesteps: 358423 Episode Num: 464 Reward: 92.12655358631775\n",
            "Total Timesteps: 359423 Episode Num: 465 Reward: 93.39134787560465\n",
            "Total Timesteps: 360423 Episode Num: 466 Reward: 92.03392458926402\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.035133\n",
            "---------------------------------------\n",
            "Total Timesteps: 361423 Episode Num: 467 Reward: 92.28584526925836\n",
            "Total Timesteps: 362423 Episode Num: 468 Reward: 92.57816943565669\n",
            "Total Timesteps: 363423 Episode Num: 469 Reward: 92.07634484169097\n",
            "Total Timesteps: 364423 Episode Num: 470 Reward: 143.79949192636246\n",
            "Total Timesteps: 365423 Episode Num: 471 Reward: 88.93275037886475\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.952815\n",
            "---------------------------------------\n",
            "Total Timesteps: 366423 Episode Num: 472 Reward: 93.74376671872074\n",
            "Total Timesteps: 367423 Episode Num: 473 Reward: 93.71007417145104\n",
            "Total Timesteps: 368423 Episode Num: 474 Reward: 92.95925697966945\n",
            "Total Timesteps: 369423 Episode Num: 475 Reward: 93.12502740193838\n",
            "Total Timesteps: 370423 Episode Num: 476 Reward: 91.81258057924889\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 93.075801\n",
            "---------------------------------------\n",
            "Total Timesteps: 371423 Episode Num: 477 Reward: 92.39498104378052\n",
            "Total Timesteps: 372423 Episode Num: 478 Reward: 90.71111323688697\n",
            "Total Timesteps: 373423 Episode Num: 479 Reward: 91.97714236820362\n",
            "Total Timesteps: 374423 Episode Num: 480 Reward: 93.98532218320965\n",
            "Total Timesteps: 375423 Episode Num: 481 Reward: 93.42770735999122\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.023522\n",
            "---------------------------------------\n",
            "Total Timesteps: 376423 Episode Num: 482 Reward: 95.51636252822652\n",
            "Total Timesteps: 377423 Episode Num: 483 Reward: 92.01072072412494\n",
            "Total Timesteps: 378423 Episode Num: 484 Reward: 96.43450083175027\n",
            "Total Timesteps: 379423 Episode Num: 485 Reward: 92.41309950831254\n",
            "Total Timesteps: 380423 Episode Num: 486 Reward: 92.19472555053876\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 94.093128\n",
            "---------------------------------------\n",
            "Total Timesteps: 381423 Episode Num: 487 Reward: 92.29707757870787\n",
            "Total Timesteps: 382423 Episode Num: 488 Reward: 89.70107252353898\n",
            "Total Timesteps: 383423 Episode Num: 489 Reward: 97.24361958304226\n",
            "Total Timesteps: 384423 Episode Num: 490 Reward: 104.3441416796765\n",
            "Total Timesteps: 385423 Episode Num: 491 Reward: 117.10476093163447\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 94.452971\n",
            "---------------------------------------\n",
            "Total Timesteps: 386423 Episode Num: 492 Reward: 88.519171861117\n",
            "Total Timesteps: 387423 Episode Num: 493 Reward: 94.45206385263604\n",
            "Total Timesteps: 388423 Episode Num: 494 Reward: 94.6340263489667\n",
            "Total Timesteps: 389423 Episode Num: 495 Reward: 92.42002409090372\n",
            "Total Timesteps: 390423 Episode Num: 496 Reward: 92.82624661768038\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.270967\n",
            "---------------------------------------\n",
            "Total Timesteps: 391423 Episode Num: 497 Reward: 90.35160300885623\n",
            "Total Timesteps: 392423 Episode Num: 498 Reward: 95.71741317261022\n",
            "Total Timesteps: 393423 Episode Num: 499 Reward: 93.05008410035359\n",
            "Total Timesteps: 394423 Episode Num: 500 Reward: 92.83194678956113\n",
            "Total Timesteps: 395423 Episode Num: 501 Reward: 94.7786994668284\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.064032\n",
            "---------------------------------------\n",
            "Total Timesteps: 396423 Episode Num: 502 Reward: 95.34897060589911\n",
            "Total Timesteps: 397423 Episode Num: 503 Reward: 95.19464694811543\n",
            "Total Timesteps: 398423 Episode Num: 504 Reward: 91.73424068610247\n",
            "Total Timesteps: 399423 Episode Num: 505 Reward: 91.23610772893603\n",
            "Total Timesteps: 400423 Episode Num: 506 Reward: 90.97237698495522\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.245961\n",
            "---------------------------------------\n",
            "Total Timesteps: 401423 Episode Num: 507 Reward: 91.9964811349453\n",
            "Total Timesteps: 402423 Episode Num: 508 Reward: 94.67481262327885\n",
            "Total Timesteps: 403423 Episode Num: 509 Reward: 93.96397576881462\n",
            "Total Timesteps: 404423 Episode Num: 510 Reward: 90.66341983807476\n",
            "Total Timesteps: 405423 Episode Num: 511 Reward: 92.75513725198324\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.548281\n",
            "---------------------------------------\n",
            "Total Timesteps: 406423 Episode Num: 512 Reward: 94.45242363522965\n",
            "Total Timesteps: 407423 Episode Num: 513 Reward: 89.36918715781229\n",
            "Total Timesteps: 408423 Episode Num: 514 Reward: 89.69833279244604\n",
            "Total Timesteps: 409423 Episode Num: 515 Reward: 94.47432281805763\n",
            "Total Timesteps: 410423 Episode Num: 516 Reward: 91.74758591253537\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.857529\n",
            "---------------------------------------\n",
            "Total Timesteps: 411423 Episode Num: 517 Reward: 103.75780245775675\n",
            "Total Timesteps: 412423 Episode Num: 518 Reward: 93.04359045153377\n",
            "Total Timesteps: 413423 Episode Num: 519 Reward: 93.28089482268344\n",
            "Total Timesteps: 414423 Episode Num: 520 Reward: 92.28218167166558\n",
            "Total Timesteps: 415423 Episode Num: 521 Reward: 107.99284599329357\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 90.238291\n",
            "---------------------------------------\n",
            "Total Timesteps: 416423 Episode Num: 522 Reward: 91.76570292451648\n",
            "Total Timesteps: 417423 Episode Num: 523 Reward: 94.09663321557382\n",
            "Total Timesteps: 418423 Episode Num: 524 Reward: 92.71025550030795\n",
            "Total Timesteps: 419423 Episode Num: 525 Reward: 98.04102517135334\n",
            "Total Timesteps: 420423 Episode Num: 526 Reward: 88.79703227213322\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.767566\n",
            "---------------------------------------\n",
            "Total Timesteps: 421423 Episode Num: 527 Reward: 95.68026909316181\n",
            "Total Timesteps: 422423 Episode Num: 528 Reward: 94.41131007570206\n",
            "Total Timesteps: 423423 Episode Num: 529 Reward: 94.04924910185852\n",
            "Total Timesteps: 424423 Episode Num: 530 Reward: 123.00025100660605\n",
            "Total Timesteps: 425423 Episode Num: 531 Reward: 93.52213091545833\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.518195\n",
            "---------------------------------------\n",
            "Total Timesteps: 426423 Episode Num: 532 Reward: 93.82775539001142\n",
            "Total Timesteps: 427423 Episode Num: 533 Reward: 92.02339410011056\n",
            "Total Timesteps: 428423 Episode Num: 534 Reward: 92.51085743144263\n",
            "Total Timesteps: 429423 Episode Num: 535 Reward: 93.47254083720333\n",
            "Total Timesteps: 430423 Episode Num: 536 Reward: 94.05268706257256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.192221\n",
            "---------------------------------------\n",
            "Total Timesteps: 431423 Episode Num: 537 Reward: 98.82907666945692\n",
            "Total Timesteps: 432423 Episode Num: 538 Reward: 90.28932755444323\n",
            "Total Timesteps: 433423 Episode Num: 539 Reward: 93.53045039397388\n",
            "Total Timesteps: 434423 Episode Num: 540 Reward: 131.6887694801613\n",
            "Total Timesteps: 435423 Episode Num: 541 Reward: 141.59028253629123\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.245587\n",
            "---------------------------------------\n",
            "Total Timesteps: 436423 Episode Num: 542 Reward: 93.84674511911227\n",
            "Total Timesteps: 437423 Episode Num: 543 Reward: 133.26330908018386\n",
            "Total Timesteps: 438423 Episode Num: 544 Reward: 94.57177700754193\n",
            "Total Timesteps: 439423 Episode Num: 545 Reward: 90.42949689501671\n",
            "Total Timesteps: 440423 Episode Num: 546 Reward: 91.53893841810333\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 88.983825\n",
            "---------------------------------------\n",
            "Total Timesteps: 441423 Episode Num: 547 Reward: 96.3214778502227\n",
            "Total Timesteps: 442423 Episode Num: 548 Reward: 94.5252227741287\n",
            "Total Timesteps: 443423 Episode Num: 549 Reward: 92.93676680453497\n",
            "Total Timesteps: 444423 Episode Num: 550 Reward: 96.95951471993874\n",
            "Total Timesteps: 445423 Episode Num: 551 Reward: 93.2135326741732\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.860524\n",
            "---------------------------------------\n",
            "Total Timesteps: 446423 Episode Num: 552 Reward: 93.39177896069373\n",
            "Total Timesteps: 447423 Episode Num: 553 Reward: 93.88177364449119\n",
            "Total Timesteps: 448423 Episode Num: 554 Reward: 91.98337147974732\n",
            "Total Timesteps: 449423 Episode Num: 555 Reward: 94.11362107780973\n",
            "Total Timesteps: 450423 Episode Num: 556 Reward: 93.83186756543223\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.549656\n",
            "---------------------------------------\n",
            "Total Timesteps: 451423 Episode Num: 557 Reward: 93.82002266405841\n",
            "Total Timesteps: 452423 Episode Num: 558 Reward: 90.21353929360487\n",
            "Total Timesteps: 453423 Episode Num: 559 Reward: 140.47166912368172\n",
            "Total Timesteps: 454423 Episode Num: 560 Reward: 92.90553341177325\n",
            "Total Timesteps: 455423 Episode Num: 561 Reward: 92.05004627289453\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 93.035045\n",
            "---------------------------------------\n",
            "Total Timesteps: 456423 Episode Num: 562 Reward: 90.32126915550542\n",
            "Total Timesteps: 457423 Episode Num: 563 Reward: 96.12296298059924\n",
            "Total Timesteps: 458423 Episode Num: 564 Reward: 92.44616516974392\n",
            "Total Timesteps: 459423 Episode Num: 565 Reward: 87.93601749620494\n",
            "Total Timesteps: 460423 Episode Num: 566 Reward: 93.08088421095333\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.911109\n",
            "---------------------------------------\n",
            "Total Timesteps: 461423 Episode Num: 567 Reward: 90.43704362758712\n",
            "Total Timesteps: 462423 Episode Num: 568 Reward: 138.94549397825546\n",
            "Total Timesteps: 463423 Episode Num: 569 Reward: 88.68724545016823\n",
            "Total Timesteps: 464423 Episode Num: 570 Reward: 98.20873860578564\n",
            "Total Timesteps: 465423 Episode Num: 571 Reward: 89.75959521505044\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 92.932999\n",
            "---------------------------------------\n",
            "Total Timesteps: 466423 Episode Num: 572 Reward: 90.80030144374182\n",
            "Total Timesteps: 467423 Episode Num: 573 Reward: 94.51042164450254\n",
            "Total Timesteps: 468423 Episode Num: 574 Reward: 94.17546236808982\n",
            "Total Timesteps: 469423 Episode Num: 575 Reward: 91.54481612069495\n",
            "Total Timesteps: 470423 Episode Num: 576 Reward: 89.73921866180355\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.434720\n",
            "---------------------------------------\n",
            "Total Timesteps: 471423 Episode Num: 577 Reward: 94.17188788090013\n",
            "Total Timesteps: 472423 Episode Num: 578 Reward: 91.44095528388141\n",
            "Total Timesteps: 473423 Episode Num: 579 Reward: 91.79796129946777\n",
            "Total Timesteps: 474423 Episode Num: 580 Reward: 94.87189122060077\n",
            "Total Timesteps: 475423 Episode Num: 581 Reward: 94.5641744125845\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.951028\n",
            "---------------------------------------\n",
            "Total Timesteps: 476423 Episode Num: 582 Reward: 92.70639273101396\n",
            "Total Timesteps: 477423 Episode Num: 583 Reward: 92.81685232980848\n",
            "Total Timesteps: 478423 Episode Num: 584 Reward: 91.48280014232955\n",
            "Total Timesteps: 479423 Episode Num: 585 Reward: 97.26322175288225\n",
            "Total Timesteps: 480423 Episode Num: 586 Reward: 94.58719710067328\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.631163\n",
            "---------------------------------------\n",
            "Total Timesteps: 481423 Episode Num: 587 Reward: 93.01550021387365\n",
            "Total Timesteps: 482423 Episode Num: 588 Reward: 95.00945300504912\n",
            "Total Timesteps: 483423 Episode Num: 589 Reward: 91.87443986247828\n",
            "Total Timesteps: 484423 Episode Num: 590 Reward: 122.72256636072834\n",
            "Total Timesteps: 485423 Episode Num: 591 Reward: 100.32805683887506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 93.025792\n",
            "---------------------------------------\n",
            "Total Timesteps: 486423 Episode Num: 592 Reward: 102.23822507769867\n",
            "Total Timesteps: 487423 Episode Num: 593 Reward: 93.81501238071911\n",
            "Total Timesteps: 488423 Episode Num: 594 Reward: 94.37241853176585\n",
            "Total Timesteps: 489423 Episode Num: 595 Reward: 90.88334103941271\n",
            "Total Timesteps: 490423 Episode Num: 596 Reward: 91.64273126554752\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 83.406271\n",
            "---------------------------------------\n",
            "Total Timesteps: 491423 Episode Num: 597 Reward: 96.77761498171793\n",
            "Total Timesteps: 492423 Episode Num: 598 Reward: 90.91281587119933\n",
            "Total Timesteps: 493423 Episode Num: 599 Reward: 133.22841712865224\n",
            "Total Timesteps: 494423 Episode Num: 600 Reward: 93.8263952045546\n",
            "Total Timesteps: 495423 Episode Num: 601 Reward: 91.3835447377194\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 87.264250\n",
            "---------------------------------------\n",
            "Total Timesteps: 496423 Episode Num: 602 Reward: 98.65291593326319\n",
            "Total Timesteps: 497423 Episode Num: 603 Reward: 132.70762317635658\n",
            "Total Timesteps: 498423 Episode Num: 604 Reward: 94.05422322758805\n",
            "Total Timesteps: 499423 Episode Num: 605 Reward: 90.93447934057667\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 97.049166\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWYvbY3lpZAT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "bd968295-93ab-4f86-a0ba-3711a18eca90"
      },
      "source": [
        " import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.plot(range(1,606), R_count, color='blue', ls='dashed', marker='o', markerfacecolor='red')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f07e4cda590>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXlojKedqDBI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bNE2l1NgcBA"
      },
      "source": [
        "# Inference (Final)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3D3qhpIgeiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56046a5b-5f37-465b-93ed-ceb059df7eac"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 85.555209\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT_nEGjOh4i-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}